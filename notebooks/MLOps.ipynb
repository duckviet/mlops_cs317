{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07089e2a-5cf2-43dd-be28-3c8075bcad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cudf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862dd498-0265-4b93-af87-ee488e00c434",
   "metadata": {},
   "source": [
    "### Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76541312-56ce-4426-8447-b5606a8ef0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/nguyenvietduc-22520273/.rs_datasets'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335867be-ffe8-417b-8441-f0ead0e8b912",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db26b0e0-a7fa-4782-a295-697271e88030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Load Time: 8.3651 seconds\n",
      "cuDF Test Load Time: 0.5362 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define data folder and file paths\n",
    "train_file = os.path.join(file_path, \"yoochoose/yoochoose-clicks.dat\")\n",
    "\n",
    "# Load data with cudf\n",
    "start_time = time.time()\n",
    "train_gdf = cudf.read_csv(\n",
    "    train_file,\n",
    "    names=['session_id', 'time', 'item_id', 'category'],\n",
    "    dtype={'session_id': 'int32', 'item_id': 'int32', 'category': 'str'},\n",
    "    parse_dates=['time']\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"cuDF Load Time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "test_file = os.path.join(file_path, \"yoochoose/yoochoose-test.dat\")\n",
    "\n",
    "# Load test data with cudf\n",
    "start_time = time.time()\n",
    "test_gdf = cudf.read_csv(\n",
    "    test_file,\n",
    "    names=['session_id', 'time', 'item_id', 'category'],\n",
    "    dtype={'session_id': 'int32', 'item_id': 'int32', 'category': 'str'},\n",
    "    parse_dates=['time']\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"cuDF Test Load Time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd3ebb-81f9-4847-83ca-3d0fa755ea9d",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ce349b-2b88-4de4-9ffb-35349a5053d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 52739\n",
      "Number of sessions: 9249729\n",
      "Number of test sessions: 2311994\n"
     ]
    }
   ],
   "source": [
    "# Sort by session_id and time\n",
    "train_gdf = train_gdf.sort_values(['session_id', 'time'])\n",
    "\n",
    "test_gdf = test_gdf.sort_values(['session_id', 'time'])\n",
    "\n",
    "# Encode item_id using cudf and cupy\n",
    "unique_items = train_gdf['item_id'].unique()  # Vẫn trên GPU\n",
    "item_encoder = cudf.Series(cp.arange(1, len(unique_items) + 1), index=unique_items)  # Tạo mapping trên GPU\n",
    "train_gdf['item_idx'] = train_gdf['item_id'].map(item_encoder)  # Ánh xạ trực tiếp trên GPU\n",
    "\n",
    "test_gdf['item_idx'] = test_gdf['item_id'].map(item_encoder) # Với item_encoder đã được tạo từ train dưới dạng cudf.Series\n",
    "test_gdf = test_gdf.dropna(subset=['item_idx'])\n",
    "test_gdf['item_idx'] = test_gdf['item_idx'].astype('int32')\n",
    "\n",
    "# Group by session_id to create sequences\n",
    "sessions_gdf = train_gdf.groupby('session_id')['item_idx'].agg('collect')  # 'collect' thay cho 'list' trong cudf\n",
    "sessions = sessions_gdf.to_arrow().to_pylist()  # Chuyển sang list Python, nhưng chỉ ở bước cuối\n",
    "\n",
    "test_sessions_gdf = test_gdf.groupby('session_id')['item_idx'].agg('collect')\n",
    "test_sessions = test_sessions_gdf.to_arrow().to_pylist()\n",
    "\n",
    "# Number of unique items and sessions\n",
    "print(f\"Number of unique items: {len(unique_items)}\")\n",
    "print(f\"Number of sessions: {len(sessions)}\")\n",
    "print(f\"Number of test sessions: {len(test_sessions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae7bd20-1a5c-4e26-8dde-e0c4058c884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SessionDataset(Dataset):\n",
    "    def __init__(self, sessions, max_len=5):\n",
    "        self.sessions = sessions\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        session = session[-self.max_len:]  # Limit session length\n",
    "        input_seq = [0] * (self.max_len - len(session)) + session[:-1]  # Padding + input sequence\n",
    "        target = session[-1]  # Target item\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "max_len = 20  # Adjust based on your needs or dataset characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d448f2-b659-4897-81ed-994e7f8e2311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created with 9249729 sessions\n",
      "Test dataset created with 2311994 sessions\n"
     ]
    }
   ],
   "source": [
    "# Tạo test dataset\n",
    "dataset = SessionDataset(sessions, max_len=max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)  # Larger batch size for bigger dataset\n",
    "print(f\"Train dataset created with {len(dataset)} sessions\")\n",
    "\n",
    "# Tạo test dataset\n",
    "test_dataset = SessionDataset(test_sessions, max_len=max_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "print(f\"Test dataset created with {len(test_dataset)} sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69aeabbd-0bd3-4e64-aaa9-673695cce5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Đường dẫn thư mục\n",
    "save_dir = '/mnt/c/Users/HP/mlops_cs317/data/processed/'\n",
    "\n",
    "# 1. Lưu item_encoder (cudf.Series) dưới dạng pickle\n",
    "with open(save_dir + 'item_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(item_encoder.to_pandas(), f)  # convert về pandas để pickle\n",
    "\n",
    "# 2. Lưu train_gdf và test_gdf dưới dạng .parquet\n",
    "train_gdf.to_parquet(save_dir + 'clicks_train.parquet', index=False)\n",
    "test_gdf.to_parquet(save_dir + 'clicks_test.parquet', index=False)\n",
    "\n",
    "# 3. Lưu sessions và test_sessions dưới dạng pickle (.pkl)\n",
    "with open(save_dir + 'train_sessions.pkl', 'wb') as f:\n",
    "    pickle.dump(sessions, f)\n",
    "\n",
    "with open(save_dir + 'test_sessions.pkl', 'wb') as f:\n",
    "    pickle.dump(test_sessions, f)\n",
    "\n",
    "print(\"Saved all successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71568ae1-ef4f-4e06-9ed9-a1cfbd79b62b",
   "metadata": {},
   "source": [
    "### SASREC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511f7bf9-8019-48be-9d78-af1a335950b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SASRec model\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, hidden_size, num_heads, num_layers, dropout=0.1):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoding = self._generate_pos_encoding(embedding_dim, max_len=100)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_size, \n",
    "                dropout=dropout, batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, num_items + 1)\n",
    "    \n",
    "    def _generate_pos_encoding(self, embedding_dim, max_len):\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "        pos_encoding = torch.zeros(max_len, embedding_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
    "        return pos_encoding.unsqueeze(0).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "        mask = (x.sum(dim=-1) == 0).bool()  # Mask for padding\n",
    "        output = self.transformer(x, src_key_padding_mask=mask)\n",
    "        output = self.fc(output[:, -1, :])  # Predict next item\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f240ee0-5cd8-460c-a887-e79408c14182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def negative_sampling_loss(outputs, targets, num_negatives=100, num_items=52739):\n",
    "    batch_size = targets.size(0)\n",
    "    \n",
    "    # Logits của item đúng (positive)\n",
    "    positive_logits = outputs[range(batch_size), targets]  # [batch_size]\n",
    "    \n",
    "    # Tạo chỉ số negative samples ngẫu nhiên\n",
    "    negative_indices = torch.randint(1, num_items + 1, (batch_size, num_negatives)).cuda()  # [batch_size, num_negatives]\n",
    "    \n",
    "    # Lấy logits của negative samples\n",
    "    # Mở rộng outputs để lập chỉ mục đúng\n",
    "    negative_logits = outputs.gather(1, negative_indices)  # [batch_size, num_negatives]\n",
    "    \n",
    "    # Tính loss: log-sigmoid của positive + sum(log-sigmoid của negatives)\n",
    "    loss = -torch.mean(F.logsigmoid(positive_logits) + torch.sum(F.logsigmoid(-negative_logits), dim=1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901988b7-d3b2-414a-8dda-adefcc8aa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SASRec(\n",
    "    num_items=52739, \n",
    "    embedding_dim=64,  # Increased embedding size for larger dataset\n",
    "    hidden_size=128,   # Increased hidden size\n",
    "    num_heads=4, \n",
    "    num_layers=2\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58793772-f6e4-44d2-a0db-bdefed885f51",
   "metadata": {},
   "source": [
    "### Evaluation Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9408652a-101e-48c1-bed6-bfe4e219ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá trên test set\n",
    "def evaluate(model, test_dataloader, num_items):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_dataloader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = negative_sampling_loss(outputs, targets, num_negatives=100, num_items=num_items)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    return avg_loss\n",
    "    \n",
    "def recall_at_20(model, dataloader, num_items, k=20):\n",
    "    model.eval()\n",
    "    total_recall = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, top_k = torch.topk(outputs, k, dim=-1)\n",
    "            matches = (top_k == targets.unsqueeze(-1)).sum(dim=-1)\n",
    "            total_recall += matches.sum().item()\n",
    "            num_samples += targets.numel()\n",
    "    return total_recall / num_samples\n",
    "    \n",
    "def mrr_at_k(model, dataloader, num_items, k=20):\n",
    "    model.eval()\n",
    "    total_mrr = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, top_k = torch.topk(outputs, k, dim=-1)\n",
    "            for i, target in enumerate(targets):\n",
    "                rank = (top_k[i] == target).nonzero(as_tuple=True)[0]\n",
    "                if rank.numel() > 0:\n",
    "                    total_mrr += 1.0 / (rank.item() + 1)\n",
    "            num_samples += targets.size(0)\n",
    "    return total_mrr / num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a3462-fa53-4c27-8fd6-bf654d7295e8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6310064c-5f9b-4772-9901-5053038587be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/12 09:34:33 INFO mlflow.tracking.fluent: Experiment with name 'YooChoose_SASRec_Tracking' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100/72264, Loss: 13.5384\n",
      "Epoch 1, Batch 200/72264, Loss: 5.6604\n",
      "Epoch 1, Batch 300/72264, Loss: 4.2298\n",
      "Epoch 1, Batch 400/72264, Loss: 4.1321\n",
      "Epoch 1, Batch 500/72264, Loss: 3.7262\n",
      "Epoch 1, Batch 600/72264, Loss: 3.9042\n",
      "Epoch 1, Batch 700/72264, Loss: 3.7726\n",
      "Epoch 1, Batch 800/72264, Loss: 3.5049\n",
      "Epoch 1, Batch 900/72264, Loss: 3.5673\n",
      "Epoch 1, Batch 1000/72264, Loss: 3.6415\n",
      "Epoch 1, Batch 1100/72264, Loss: 3.7526\n",
      "Epoch 1, Batch 1200/72264, Loss: 3.6199\n",
      "Epoch 1, Batch 1300/72264, Loss: 3.5575\n",
      "Epoch 1, Batch 1400/72264, Loss: 3.9104\n",
      "Epoch 1, Batch 1500/72264, Loss: 3.5077\n",
      "Epoch 1, Batch 1600/72264, Loss: 3.5615\n",
      "Epoch 1, Batch 1700/72264, Loss: 3.4799\n",
      "Epoch 1, Batch 1800/72264, Loss: 3.6982\n",
      "Epoch 1, Batch 1900/72264, Loss: 3.6894\n",
      "Epoch 1, Batch 2000/72264, Loss: 3.6175\n",
      "Epoch 1, Batch 2100/72264, Loss: 3.5000\n",
      "Epoch 1, Batch 2200/72264, Loss: 3.4162\n",
      "Epoch 1, Batch 2300/72264, Loss: 3.2168\n",
      "Epoch 1, Batch 2400/72264, Loss: 3.5162\n",
      "Epoch 1, Batch 2500/72264, Loss: 3.2582\n",
      "Epoch 1, Batch 2600/72264, Loss: 3.4339\n",
      "Epoch 1, Batch 2700/72264, Loss: 3.2100\n",
      "Epoch 1, Batch 2800/72264, Loss: 3.3604\n",
      "Epoch 1, Batch 2900/72264, Loss: 3.2905\n",
      "Epoch 1, Batch 3000/72264, Loss: 3.0930\n",
      "Epoch 1, Batch 3100/72264, Loss: 3.4129\n",
      "Epoch 1, Batch 3200/72264, Loss: 2.9652\n",
      "Epoch 1, Batch 3300/72264, Loss: 3.0738\n",
      "Epoch 1, Batch 3400/72264, Loss: 3.1966\n",
      "Epoch 1, Batch 3500/72264, Loss: 3.1109\n",
      "Epoch 1, Batch 3600/72264, Loss: 2.9762\n",
      "Epoch 1, Batch 3700/72264, Loss: 2.9429\n",
      "Epoch 1, Batch 3800/72264, Loss: 2.6741\n",
      "Epoch 1, Batch 3900/72264, Loss: 2.9903\n",
      "Epoch 1, Batch 4000/72264, Loss: 2.8843\n",
      "Epoch 1, Batch 4100/72264, Loss: 2.9862\n",
      "Epoch 1, Batch 4200/72264, Loss: 3.2445\n",
      "Epoch 1, Batch 4300/72264, Loss: 3.0036\n",
      "Epoch 1, Batch 4400/72264, Loss: 3.1467\n",
      "Epoch 1, Batch 4500/72264, Loss: 2.7564\n",
      "Epoch 1, Batch 4600/72264, Loss: 2.7342\n",
      "Epoch 1, Batch 4700/72264, Loss: 2.7429\n",
      "Epoch 1, Batch 4800/72264, Loss: 3.1896\n",
      "Epoch 1, Batch 4900/72264, Loss: 2.8502\n",
      "Epoch 1, Batch 5000/72264, Loss: 2.9230\n",
      "Epoch 1, Batch 5100/72264, Loss: 2.4980\n",
      "Epoch 1, Batch 5200/72264, Loss: 2.4888\n",
      "Epoch 1, Batch 5300/72264, Loss: 2.8517\n",
      "Epoch 1, Batch 5400/72264, Loss: 2.6902\n",
      "🏃 View run SASRec_NegativeSampling at: http://192.168.28.39:5000/#/experiments/361798885674694128/runs/ac9a7a872d024085bf9d10a6ca96afdb\n",
      "🧪 View experiment at: http://192.168.28.39:5000/#/experiments/361798885674694128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Thiết lập MLFlow\n",
    "mlflow.set_tracking_uri(\"http://192.168.28.39:5000\")\n",
    "mlflow.set_experiment(\"YooChoose_SASRec_Tracking\")\n",
    "\n",
    "# Training loop với MLFlow và early stopping\n",
    "num_epochs = 2\n",
    "num_items = 52739\n",
    "learning_rate = 0.001\n",
    "num_negatives = 100\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "with mlflow.start_run(run_name=\"SASRec_NegativeSampling\"):\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"num_items\", num_items)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"num_negatives\", num_negatives)\n",
    "    mlflow.log_param(\"patience\", patience)\n",
    "    mlflow.log_param(\"batch_size\", dataloader.batch_size)\n",
    "    mlflow.log_param(\"max_len\", 5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = negative_sampling_loss(outputs, targets, num_negatives=num_negatives, num_items=num_items)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Time: {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # Evaluate\n",
    "        avg_test_loss = evaluate(model, test_dataloader, num_items)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg Test Loss: {avg_test_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"test_loss\", avg_test_loss, step=epoch)\n",
    "        # mlflow.log_metric(\"recall@20\", recall_20, step=epoch)\n",
    "        # mlflow.log_metric(\"mrr@20\", mrr_20, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_time\", epoch_time, step=epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(), 'loss': best_loss}, \n",
    "                       checkpoint_path)\n",
    "            mlflow.log_artifact(checkpoint_path)\n",
    "            print(f\"Saved best model at Epoch {epoch+1} with Test Loss: {best_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after Epoch {epoch+1}. Best Test Loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    mlflow.pytorch.log_model(model, \"final_model\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Loaded best model from {checkpoint_path} with Test Loss: {checkpoint['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d2c31-fd72-4a24-80a0-a66efd450ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
